{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8ff0ac1-ab83-404a-b55a-f3e271a5bb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "train_dir = 'C:/Users/ouaha/OneDrive/Bureau/projet app stat/archive/data/Training'  # Remplacez par le chemin réel du dossier d'entraînement\n",
    "test_dir = \"C:/Users/ouaha/OneDrive/Bureau/projet app stat/archive/data/Testing\"         # Remplacez par le chemin réel du dossier de test\n",
    "with open('C:/Users/ouaha/OneDrive/Bureau/projet app stat/archive/config.yml', 'r') as file:\n",
    "    params = yaml.safe_load(file)\n",
    "\n",
    "preprocessing_params = params['preprocessing']\n",
    "target_size = tuple(preprocessing_params['target_size'])\n",
    "normalize = preprocessing_params['normalize']\n",
    "standardize = preprocessing_params['standardize']\n",
    "grayscale = preprocessing_params['grayscale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcfc59fa-fb87-4c2a-89d6-48486aee99ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(path):\n",
    "    # Charger l'image\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_image(img, channels=1 if grayscale else 3)  # Convertir en niveaux de gris si spécifié\n",
    "    img = tf.image.resize(img, target_size)  # Redimensionner l'image\n",
    "\n",
    "    if normalize:\n",
    "        img = img / 255.0  # Normaliser entre 0 et 1\n",
    "\n",
    "    if standardize:\n",
    "        img = (img - tf.reduce_mean(img)) / tf.math.reduce_std(img)  # Standardisation\n",
    "\n",
    "    return img\n",
    "\n",
    "def prepare_dataset(directory):\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for label in os.listdir(directory):\n",
    "        label_dir = os.path.join(directory, label)\n",
    "        if os.path.isdir(label_dir):\n",
    "            for img_name in os.listdir(label_dir):\n",
    "                img_path = os.path.join(label_dir, img_name)\n",
    "                images.append(load_and_preprocess_image(img_path))\n",
    "                labels.append(label)  # ou utiliser un encodage d'étiquettes\n",
    "\n",
    "    return tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "\n",
    "train_dataset = prepare_dataset(train_dir)\n",
    "test_dataset = prepare_dataset(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a6f0622-f736-441e-b280-f8afeb044c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille de l'ensemble d'entraînement : 2296\n",
      "Taille de l'ensemble de validation : 574\n"
     ]
    }
   ],
   "source": [
    "full_dataset = train_dataset.shuffle(buffer_size=2000)\n",
    "\n",
    "# Définir le pourcentage pour la validation\n",
    "validation_split = 0.2  # 10% pour la validation\n",
    "num_samples = tf.data.experimental.cardinality(full_dataset).numpy()  # Nombre total d'échantillons\n",
    "num_validation = int(num_samples * validation_split)  # Nombre d'échantillons pour la validation\n",
    "\n",
    "# Créer les ensembles d'entraînement et de validation\n",
    "val_dataset = full_dataset.take(num_validation)  # Prendre les premiers num_validation échantillons\n",
    "train_dataset = full_dataset.skip(num_validation)  # Sauter ces échantillons pour l'ensemble d'entraînement\n",
    "\n",
    "print(f'Taille de l\\'ensemble d\\'entraînement : {tf.data.experimental.cardinality(train_dataset).numpy()}')\n",
    "print(f'Taille de l\\'ensemble de validation : {tf.data.experimental.cardinality(val_dataset).numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c028810-c4f6-4b45-8a8d-d8358cd08636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels présents dans l'ensemble de validation : {'pituitary_tumor', 'meningioma_tumor', 'glioma_tumor', 'no_tumor'}\n"
     ]
    }
   ],
   "source": [
    "labels_seen = set()\n",
    "\n",
    "for image, label in val_dataset:\n",
    "    label_decoded = label.numpy().decode('utf-8')\n",
    "    labels_seen.add(label_decoded)\n",
    "\n",
    "print(\"Labels présents dans l'ensemble de validation :\", labels_seen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34f0a849-dc75-49df-9ab1-0fb0c09decae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(256, 256, 3)),\n",
    "\n",
    "    # Bloc 1\n",
    "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "\n",
    "    # Bloc 2\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    # Bloc 3\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "\n",
    "    # Bloc 4\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    # Couche Fully Connected\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.2),  # Pour éviter le surapprentissage\n",
    "    layers.Dense(4, activation='softmax')\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "# Création du modèle\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8378719-af34-4fec-860f-4796c562b4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = tf.lookup.StaticHashTable(\n",
    "    initializer=tf.lookup.KeyValueTensorInitializer(\n",
    "        keys=tf.constant(['glioma_tumor', 'pituitary_tumor', 'no_tumor', 'meningioma_tumor']),\n",
    "        values=tf.constant([0, 1, 2, 3]),\n",
    "    ),\n",
    "    default_value=tf.constant(-1)  # Default value if key is missing\n",
    ")\n",
    "\n",
    "# Function to map labels using the lookup table\n",
    "def map_labels(x, y):\n",
    "    return x, label_map.lookup(y)\n",
    "\n",
    "# Apply the mapping to the dataset\n",
    "train_dataset = train_dataset.map(map_labels)\n",
    "val_dataset = val_dataset.map(map_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45451154-789e-4a2e-9f02-0ea443674dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1007s\u001b[0m 14s/step - accuracy: 0.4954 - loss: 3.7429 - val_accuracy: 0.6760 - val_loss: 0.7238\n",
      "Epoch 2/10\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1002s\u001b[0m 14s/step - accuracy: 0.6949 - loss: 0.6871 - val_accuracy: 0.7909 - val_loss: 0.5055\n",
      "Epoch 3/10\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1000s\u001b[0m 14s/step - accuracy: 0.8267 - loss: 0.4490 - val_accuracy: 0.8920 - val_loss: 0.2914\n",
      "Epoch 4/10\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1033s\u001b[0m 14s/step - accuracy: 0.8564 - loss: 0.3228 - val_accuracy: 0.9460 - val_loss: 0.1743\n",
      "Epoch 5/10\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1067s\u001b[0m 15s/step - accuracy: 0.9049 - loss: 0.2303 - val_accuracy: 0.9495 - val_loss: 0.1642\n",
      "Epoch 6/10\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1029s\u001b[0m 14s/step - accuracy: 0.9218 - loss: 0.1926 - val_accuracy: 0.9808 - val_loss: 0.0730\n",
      "Epoch 7/10\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1049s\u001b[0m 14s/step - accuracy: 0.9384 - loss: 0.1516 - val_accuracy: 0.9756 - val_loss: 0.0938\n",
      "Epoch 8/10\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1075s\u001b[0m 15s/step - accuracy: 0.9472 - loss: 0.1459 - val_accuracy: 0.9861 - val_loss: 0.0755\n",
      "Epoch 9/10\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1511s\u001b[0m 21s/step - accuracy: 0.9602 - loss: 0.1337 - val_accuracy: 0.9913 - val_loss: 0.0483\n",
      "Epoch 10/10\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1584s\u001b[0m 21s/step - accuracy: 0.9655 - loss: 0.0874 - val_accuracy: 0.9983 - val_loss: 0.0163\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.batch(32)\n",
    "val_dataset = val_dataset.batch(32)\n",
    "\n",
    "\n",
    "history=model.fit(train_dataset, epochs=10, validation_data= val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e51b8da0-4edf-40bc-a571-3acc4a51cdea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle sauvegardé dans C:/Users/ouaha/OneDrive/Bureau/projet app stat/archive/models/my_trained_model.keras\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_save_path = 'C:/Users/ouaha/OneDrive/Bureau/projet app stat/archive/models/my_trained_model.keras'\n",
    "model.save(model_save_path)\n",
    "print(f\"Modèle sauvegardé dans {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9a8904-8dbd-49f6-bc50-661ece684552",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_env)",
   "language": "python",
   "name": ".conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
